{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2018 The TensorFlow Constrained Optimization Authors. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we explore the problem of classification with fairness on the [[UCI Adult dataset]](https://archive.ics.uci.edu/ml/datasets/adult). We show how to set up a classification problem with data-dependent fairness constraints using the TensorFlow Constrained Optimization library and then subsequently train to optimize fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/neelguha/Dropbox/NeelResearch/fairness/code/tensorflow_constrained_optimization/')\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "import tensorflow_constrained_optimization as tfco\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and processing dataset.\n",
    "\n",
    "We load the [[UCI Adult dataset]](https://archive.ics.uci.edu/ml/datasets/adult) and do some pre-processing. The dataset is based on census data and the goal is to predict whether someone's income is over 50k. We construct four protected groups, two based on gender (Male and Female) and two based on race (White and Black).\n",
    "\n",
    "We preprocess the features as done in works such as [[ZafarEtAl15]](https://arxiv.org/abs/1507.05259) and [[GohEtAl16]](https://arxiv.org/abs/1606.07558). We transform the categorical features into binary ones and transform the continuous feature into buckets based on each feature's 5 quantiles values in training.\n",
    "\n",
    "The fairness goal is that of equal opportunity. That is, we would like the true positive rates of our classifier on the protected groups to match that of the overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\n",
    "    'workclass', 'education', 'marital_status', 'occupation', 'relationship',\n",
    "    'race', 'gender', 'native_country'\n",
    "]\n",
    "CONTINUOUS_COLUMNS = [\n",
    "    'age', 'capital_gain', 'capital_loss', 'hours_per_week', 'education_num'\n",
    "]\n",
    "COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "LABEL_COLUMN = 'label'\n",
    "\n",
    "PROTECTED_COLUMNS = [\n",
    "    'gender_Female', 'gender_Male', 'race_White', 'race_Black'\n",
    "]\n",
    "\n",
    "def get_data():\n",
    "    train_df_raw = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", names=COLUMNS, skipinitialspace=True)\n",
    "    test_df_raw = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", names=COLUMNS, skipinitialspace=True, skiprows=1)\n",
    "\n",
    "    train_df_raw[LABEL_COLUMN] = (train_df_raw['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
    "    test_df_raw[LABEL_COLUMN] = (test_df_raw['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)\n",
    "    # Preprocessing Features\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Functions for preprocessing categorical and continuous columns.\n",
    "    def binarize_categorical_columns(input_train_df, input_test_df, categorical_columns=[]):\n",
    "\n",
    "        def fix_columns(input_train_df, input_test_df):\n",
    "            test_df_missing_cols = set(input_train_df.columns) - set(input_test_df.columns)\n",
    "            for c in test_df_missing_cols:\n",
    "                input_test_df[c] = 0\n",
    "                train_df_missing_cols = set(input_test_df.columns) - set(input_train_df.columns)\n",
    "            for c in train_df_missing_cols:\n",
    "                input_train_df[c] = 0\n",
    "                input_train_df = input_train_df[input_test_df.columns]\n",
    "            return input_train_df, input_test_df\n",
    "\n",
    "        # Binarize categorical columns.\n",
    "        binarized_train_df = pd.get_dummies(input_train_df, columns=categorical_columns)\n",
    "        binarized_test_df = pd.get_dummies(input_test_df, columns=categorical_columns)\n",
    "        # Make sure the train and test dataframes have the same binarized columns.\n",
    "        fixed_train_df, fixed_test_df = fix_columns(binarized_train_df, binarized_test_df)\n",
    "        return fixed_train_df, fixed_test_df\n",
    "\n",
    "    def bucketize_continuous_column(input_train_df,\n",
    "                                  input_test_df,\n",
    "                                  continuous_column_name,\n",
    "                                  num_quantiles=None,\n",
    "                                  bins=None):\n",
    "        assert (num_quantiles is None or bins is None)\n",
    "        if num_quantiles is not None:\n",
    "            train_quantized, bins_quantized = pd.qcut(\n",
    "              input_train_df[continuous_column_name],\n",
    "              num_quantiles,\n",
    "              retbins=True,\n",
    "              labels=False)\n",
    "            input_train_df[continuous_column_name] = pd.cut(\n",
    "              input_train_df[continuous_column_name], bins_quantized, labels=False)\n",
    "            input_test_df[continuous_column_name] = pd.cut(\n",
    "              input_test_df[continuous_column_name], bins_quantized, labels=False)\n",
    "        elif bins is not None:\n",
    "            input_train_df[continuous_column_name] = pd.cut(\n",
    "              input_train_df[continuous_column_name], bins, labels=False)\n",
    "            input_test_df[continuous_column_name] = pd.cut(\n",
    "              input_test_df[continuous_column_name], bins, labels=False)\n",
    "\n",
    "    # Filter out all columns except the ones specified.\n",
    "    train_df = train_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + [LABEL_COLUMN]]\n",
    "    test_df = test_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + [LABEL_COLUMN]]\n",
    "    \n",
    "    # Bucketize continuous columns.\n",
    "    bucketize_continuous_column(train_df, test_df, 'age', num_quantiles=4)\n",
    "    bucketize_continuous_column(train_df, test_df, 'capital_gain', bins=[-1, 1, 4000, 10000, 100000])\n",
    "    bucketize_continuous_column(train_df, test_df, 'capital_loss', bins=[-1, 1, 1800, 1950, 4500])\n",
    "    bucketize_continuous_column(train_df, test_df, 'hours_per_week', bins=[0, 39, 41, 50, 100])\n",
    "    bucketize_continuous_column(train_df, test_df, 'education_num', bins=[0, 8, 9, 11, 16])\n",
    "    train_df, test_df = binarize_categorical_columns(train_df, test_df, categorical_columns=CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS)\n",
    "    feature_names = list(train_df.keys())\n",
    "    feature_names.remove(LABEL_COLUMN)\n",
    "    num_features = len(feature_names)\n",
    "    \n",
    "    return train_df, test_df, feature_names\n",
    "\n",
    "train_df, test_df, FEATURE_NAMES = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>workclass_?</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>workclass_State-gov</th>\n",
       "      <th>workclass_Without-pay</th>\n",
       "      <th>...</th>\n",
       "      <th>capital_loss_2</th>\n",
       "      <th>capital_loss_3</th>\n",
       "      <th>hours_per_week_0</th>\n",
       "      <th>hours_per_week_1</th>\n",
       "      <th>hours_per_week_2</th>\n",
       "      <th>hours_per_week_3</th>\n",
       "      <th>education_num_0</th>\n",
       "      <th>education_num_1</th>\n",
       "      <th>education_num_2</th>\n",
       "      <th>education_num_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  workclass_?  workclass_Federal-gov  workclass_Local-gov  \\\n",
       "0      0            0                      0                    0   \n",
       "1      0            0                      0                    0   \n",
       "2      0            0                      0                    0   \n",
       "3      0            0                      0                    0   \n",
       "4      0            0                      0                    0   \n",
       "\n",
       "   workclass_Never-worked  workclass_Private  workclass_Self-emp-inc  \\\n",
       "0                       0                  0                       0   \n",
       "1                       0                  0                       0   \n",
       "2                       0                  1                       0   \n",
       "3                       0                  1                       0   \n",
       "4                       0                  1                       0   \n",
       "\n",
       "   workclass_Self-emp-not-inc  workclass_State-gov  workclass_Without-pay  \\\n",
       "0                           0                    1                      0   \n",
       "1                           1                    0                      0   \n",
       "2                           0                    0                      0   \n",
       "3                           0                    0                      0   \n",
       "4                           0                    0                      0   \n",
       "\n",
       "        ...         capital_loss_2  capital_loss_3  hours_per_week_0  \\\n",
       "0       ...                      0               0                 0   \n",
       "1       ...                      0               0                 1   \n",
       "2       ...                      0               0                 0   \n",
       "3       ...                      0               0                 0   \n",
       "4       ...                      0               0                 0   \n",
       "\n",
       "   hours_per_week_1  hours_per_week_2  hours_per_week_3  education_num_0  \\\n",
       "0                 1                 0                 0                0   \n",
       "1                 0                 0                 0                0   \n",
       "2                 1                 0                 0                0   \n",
       "3                 1                 0                 0                1   \n",
       "4                 1                 0                 0                0   \n",
       "\n",
       "   education_num_1  education_num_2  education_num_3  \n",
       "0                0                0                1  \n",
       "1                0                0                1  \n",
       "2                1                0                0  \n",
       "3                0                0                0  \n",
       "4                0                0                1  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model.\n",
    "\n",
    "We use a linear model and predict positively or negatively based on threshold at 0.\n",
    "\n",
    "In the following code, we initialize the placeholders and model. In build_train_op, we set up the constrained optimization problem. We create a rate context for the entire dataset, and compute the overall false positive rate as the positive prediction rate on the negatively labeled subset. We then construct a constraint for each of the protected groups based on the difference between the true positive rates of the protected group and that of the overall dataset. We then construct a minimization problem using RateMinimizationProblem and use the ProxyLagrangianOptimizer as the solver. build_train_op initializes a training operation which will later be used to actually train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self,\n",
    "                 tpr_max_diff=0):\n",
    "        tf.random.set_random_seed(123)\n",
    "        self.tpr_max_diff = tpr_max_diff\n",
    "        num_features = len(FEATURE_NAMES)\n",
    "        self.features_placeholder = tf.placeholder(\n",
    "            tf.float32, shape=(None, num_features), name='features_placeholder')\n",
    "        self.labels_placeholder = tf.placeholder(\n",
    "            tf.float32, shape=(None, 1), name='labels_placeholder')\n",
    "        self.protected_placeholders = [tf.placeholder(tf.float32, shape=(None, 1), name=attribute+\"_placeholder\") for attribute in PROTECTED_COLUMNS]\n",
    "        # We use a linear model.\n",
    "        self.predictions_tensor = tf.layers.dense(inputs=self.features_placeholder, units=1, activation=None)\n",
    "\n",
    "\n",
    "    def build_train_op(self,\n",
    "                       learning_rate,\n",
    "                       unconstrained=False):\n",
    "        ctx = tfco.rate_context(self.predictions_tensor, self.labels_placeholder)\n",
    "        positive_slice = ctx.subset(self.labels_placeholder > 0) \n",
    "        overall_tpr = tfco.positive_prediction_rate(positive_slice)\n",
    "        constraints = []\n",
    "        if not unconstrained:\n",
    "            for placeholder in self.protected_placeholders:\n",
    "                slice_tpr = tfco.positive_prediction_rate(ctx.subset((placeholder > 0) & (self.labels_placeholder > 0)))\n",
    "                constraints.append(slice_tpr >= overall_tpr - self.tpr_max_diff)\n",
    "        mp = tfco.RateMinimizationProblem(tfco.error_rate(ctx), constraints)\n",
    "        opt = tfco.ProxyLagrangianOptimizer(tf.train.AdamOptimizer(learning_rate))\n",
    "        self.train_op = opt.minimize(minimization_problem=mp)\n",
    "        return self.train_op\n",
    "  \n",
    "    def feed_dict_helper(self, dataframe):\n",
    "        feed_dict = {self.features_placeholder:\n",
    "                  dataframe[FEATURE_NAMES],\n",
    "              self.labels_placeholder:\n",
    "                  dataframe[[LABEL_COLUMN]],}\n",
    "        for i, protected_attribute in enumerate(PROTECTED_COLUMNS):\n",
    "            feed_dict[self.protected_placeholders[i]] = dataframe[[protected_attribute]]\n",
    "        return feed_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training.\n",
    "\n",
    "Below is the function which performs the training of our constrained optimization problem. Each call to the function does one epoch through the dataset and then yields the training and testing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-c664ffc52547>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-c664ffc52547>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    feed_dict=model.feed_dict_helper(train_df))session.run(\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def training_generator(model,\n",
    "                       train_df,\n",
    "                       test_df,\n",
    "                       minibatch_size,\n",
    "                       num_iterations_per_loop=1,\n",
    "                       num_loops=1):\n",
    "    random.seed(31337)\n",
    "    num_rows = train_df.shape[0]\n",
    "    minibatch_size = min(minibatch_size, num_rows)\n",
    "    permutation = list(range(train_df.shape[0]))\n",
    "    random.shuffle(permutation)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run((tf.global_variables_initializer(),\n",
    "               tf.local_variables_initializer()))\n",
    "\n",
    "    minibatch_start_index = 0\n",
    "    for n in xrange(num_loops):\n",
    "        for _ in xrange(num_iterations_per_loop):\n",
    "            minibatch_indices = []\n",
    "            while len(minibatch_indices) < minibatch_size:\n",
    "                minibatch_end_index = (\n",
    "                minibatch_start_index + minibatch_size - len(minibatch_indices))\n",
    "                if minibatch_end_index >= num_rows:\n",
    "                    minibatch_indices += range(minibatch_start_index, num_rows)\n",
    "                    minibatch_start_index = 0\n",
    "                else:\n",
    "                    minibatch_indices += range(minibatch_start_index, minibatch_end_index)\n",
    "                    minibatch_start_index = minibatch_end_index\n",
    "                    \n",
    "            session.run(\n",
    "                  model.train_op,\n",
    "                  feed_dict=model.feed_dict_helper(\n",
    "                      train_df.iloc[[permutation[ii] for ii in minibatch_indices]]))\n",
    "\n",
    "        train_predictions = session.run(\n",
    "            model.predictions_tensor,\n",
    "            feed_dict=model.feed_dict_helper(train_df))\n",
    "        session.run(\n",
    "            model.predictions_tensor,\n",
    "            feed_dict=model.feed_dict_helper(train_df))\n",
    "        session.run(\n",
    "            model.predictions_tensor,\n",
    "            feed_dict=model.feed_dict_helper(train_df))\n",
    "        test_predictions = session.run(\n",
    "            model.predictions_tensor,\n",
    "            feed_dict=model.feed_dict_helper(test_df))\n",
    "\n",
    "        yield (train_predictions, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing accuracy and fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    signed_labels = (\n",
    "      (labels > 0).astype(np.float32) - (labels <= 0).astype(np.float32))\n",
    "    numerator = (np.multiply(signed_labels, predictions) <= 0).sum()\n",
    "    denominator = predictions.shape[0]\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "\n",
    "def positive_prediction_rate(predictions, subset):\n",
    "    numerator = np.multiply((predictions > 0).astype(np.float32),\n",
    "                          (subset > 0).astype(np.float32)).sum()\n",
    "    denominator = (subset > 0).sum()\n",
    "    return float(numerator) / float(denominator)\n",
    "\n",
    "def tpr(df):\n",
    "    \"\"\"Measure the true positive rate.\"\"\"\n",
    "    fp = sum((df['predictions'] >= 0.0) & (df[LABEL_COLUMN] > 0.5))\n",
    "    ln = sum(df[LABEL_COLUMN] > 0.5)\n",
    "    return float(fp) / float(ln)\n",
    "\n",
    "def _get_error_rate_and_constraints(df, tpr_max_diff):\n",
    "    \"\"\"Computes the error and fairness violations.\"\"\"\n",
    "    error_rate_local = error_rate(df[['predictions']], df[[LABEL_COLUMN]])\n",
    "    overall_tpr = tpr(df)\n",
    "    return error_rate_local, [(overall_tpr - tpr_max_diff) - tpr(df[df[protected_attribute] > 0.5]) for protected_attribute in PROTECTED_COLUMNS]\n",
    "\n",
    "def _get_exp_error_rate_constraints(cand_dist, error_rates_vector, constraints_matrix):\n",
    "    \"\"\"Computes the expected error and fairness violations on a randomized solution.\"\"\"\n",
    "    expected_error_rate = np.dot(cand_dist, error_rates_vector)\n",
    "    expected_constraints = np.matmul(cand_dist, constraints_matrix)\n",
    "    return expected_error_rate, expected_constraints\n",
    "\n",
    "def training_helper(model,\n",
    "                    train_df,\n",
    "                    test_df,\n",
    "                    minibatch_size,\n",
    "                    num_iterations_per_loop=1,\n",
    "                    num_loops=1):\n",
    "    train_error_rate_vector = []\n",
    "    train_constraints_matrix = []\n",
    "    test_error_rate_vector = []\n",
    "    test_constraints_matrix = []\n",
    "    for train, test in training_generator(\n",
    "      model, train_df, test_df, minibatch_size, num_iterations_per_loop,\n",
    "      num_loops):\n",
    "        train_df['predictions'] = train\n",
    "        test_df['predictions'] = test\n",
    "\n",
    "        train_error_rate, train_constraints = _get_error_rate_and_constraints(\n",
    "          train_df, model.tpr_max_diff)\n",
    "        train_error_rate_vector.append(train_error_rate)\n",
    "        train_constraints_matrix.append(train_constraints)\n",
    "\n",
    "        test_error_rate, test_constraints = _get_error_rate_and_constraints(\n",
    "            test_df, model.tpr_max_diff)\n",
    "        test_error_rate_vector.append(test_error_rate)\n",
    "        test_constraints_matrix.append(test_constraints)\n",
    "\n",
    "    return (train_error_rate_vector, train_constraints_matrix, test_error_rate_vector, test_constraints_matrix)\n",
    "\n",
    "def get_tpr_subset(df, subsets):\n",
    "    filtered = df \n",
    "    for subset in subsets:\n",
    "        filtered = filtered[filtered[subset] > 0]\n",
    "    return tpr(filtered)\n",
    "\n",
    "def get_acc_subset(df, subsets):\n",
    "    filtered = df \n",
    "    for subset in subsets:\n",
    "        filtered = filtered[filtered[subset] > 0]\n",
    "    predictions = filtered['predictions']\n",
    "    labels = filtered['label']\n",
    "    return np.mean(np.array(predictions > 0.0) == np.array(labels > 0.0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline without constraints.\n",
    "\n",
    "We now declare the model, build the training op, and then perform the training. We use a linear classifier, and train using the ADAM optimizer with learning rate 0.01, with minibatch size of 100 over 40 epochs. We first train without fairness constraints to show the baseline performance. We see that without training fair fairness, we obtain a high fairness violation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(tpr_max_diff=0.05)\n",
    "model.build_train_op(0.01, unconstrained=True)\n",
    "\n",
    "# training_helper returns the list of errors and violations over each epoch.\n",
    "train_errors, train_violations, test_errors, test_violations = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=326,\n",
    "      num_loops=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.14204109210405086\n",
      "Train Violation 0.01991132819062147\n",
      "\n",
      "Test Error 0.14341870892451325\n",
      "Test Violation 0.04300237931304962\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", train_errors[-1])\n",
    "print(\"Train Violation\", max(train_violations[-1]))\n",
    "print()\n",
    "print(\"Test Error\", test_errors[-1])\n",
    "print(\"Test Violation\", max(test_violations[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline overall accuracy: 0.856581\n",
      "Baseline overall TPR: 0.567863\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline overall accuracy: %f\" % (1 - error_rate(test_df['predictions'], test_df['label'])))\n",
    "print(\"Baseline overall TPR: %f\" % tpr(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = [\n",
    "    ['gender_Female'], ['gender_Male'], ['race_White'], ['race_Black'],\n",
    "    ['gender_Female', 'race_White'],\n",
    "    ['gender_Female', 'race_Black'],\n",
    "    ['gender_Male', 'race_White'],\n",
    "    ['gender_Male', 'race_Black']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gender_Female'] Accuracy: 0.9273196827153661\n",
      "['gender_Male'] Accuracy: 0.8212707182320442\n",
      "['race_White'] Accuracy: 0.8501362397820164\n",
      "['race_Black'] Accuracy: 0.9122357463164638\n",
      "['gender_Female', 'race_White'] Accuracy: 0.9229190421892817\n",
      "['gender_Female', 'race_Black'] Accuracy: 0.9561752988047809\n",
      "['gender_Male', 'race_White'] Accuracy: 0.8167555695010982\n",
      "['gender_Male', 'race_Black'] Accuracy: 0.8712871287128713\n",
      "\n",
      "['gender_Female'] TPR: 0.5101694915254237\n",
      "['gender_Male'] TPR: 0.5783169533169533\n",
      "['race_White'] TPR: 0.5724928366762178\n",
      "['race_Black'] TPR: 0.4748603351955307\n",
      "['gender_Female', 'race_White'] TPR: 0.5097276264591439\n",
      "['gender_Female', 'race_Black'] TPR: 0.47619047619047616\n",
      "['gender_Male', 'race_White'] TPR: 0.5833333333333334\n",
      "['gender_Male', 'race_Black'] TPR: 0.4744525547445255\n"
     ]
    }
   ],
   "source": [
    "for subset in subsets:\n",
    "    acc = get_acc_subset(test_df, subset)\n",
    "    print(subset, \"Accuracy:\", acc)\n",
    "print()\n",
    "for subset in subsets:\n",
    "    tpr_val = get_tpr_subset(test_df, subset)\n",
    "    print(subset, \"TPR:\", tpr_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with fairness constraints.\n",
    "\n",
    "We now show train with the constraints using the procedure of [[CoJiSr19]](https://arxiv.org/abs/1804.06500) and returning the last solution found. We see that the fairness violation improves.\n",
    "\n",
    "We allow an additive fairness slack of 0.05. That is, when training and evaluating the fairness constraints, the true positive rate difference between protected group has to be at least that of the overall dataset up to a slack of at most 0.05. Thus, the fairness constraints would be of the form TPR_p >= TPR - 0.05, where TPR_p and TPR denotes the true positive rates of the protected group and the overall dataset, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(tpr_max_diff=0.01)\n",
    "model.build_train_op(0.01, unconstrained=False)\n",
    "\n",
    "# training_helper returns the list of errors and violations over each epoch.\n",
    "train_errors, train_violations, test_errors, test_violations = training_helper(\n",
    "      model,\n",
    "      train_df,\n",
    "      test_df,\n",
    "      100,\n",
    "      num_iterations_per_loop=326,\n",
    "      num_loops=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.14326955560332913\n",
      "Train Violation 0.0056172401940769445\n",
      "\n",
      "Test Error 0.14495424113997912\n",
      "Test Violation 0.051942030753855895\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", train_errors[-1])\n",
    "print(\"Train Violation\", max(train_violations[-1]))\n",
    "print()\n",
    "print(\"Test Error\", test_errors[-1])\n",
    "print(\"Test Violation\", max(test_violations[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline overall accuracy: 0.855046\n",
      "Baseline overall TPR: 0.553562\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline overall accuracy: %f\" % (1 - error_rate(test_df['predictions'], test_df['label'])))\n",
    "print(\"Baseline overall TPR: %f\" % tpr(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gender_Female'] Accuracy: 0.9260284080427965\n",
      "['gender_Male'] Accuracy: 0.8196132596685083\n",
      "['race_White'] Accuracy: 0.8489172522587122\n",
      "['race_Black'] Accuracy: 0.9064702114029468\n",
      "['gender_Female', 'race_White'] Accuracy: 0.9231470923603192\n",
      "['gender_Female', 'race_Black'] Accuracy: 0.9468791500664011\n",
      "['gender_Male', 'race_White'] Accuracy: 0.8148729212425478\n",
      "['gender_Male', 'race_Black'] Accuracy: 0.8688118811881188\n",
      "\n",
      "['gender_Female'] TPR: 0.5474576271186441\n",
      "['gender_Male'] TPR: 0.5546683046683046\n",
      "['race_White'] TPR: 0.5570200573065902\n",
      "['race_Black'] TPR: 0.49162011173184356\n",
      "['gender_Female', 'race_White'] TPR: 0.5505836575875487\n",
      "['gender_Female', 'race_Black'] TPR: 0.5238095238095238\n",
      "['gender_Male', 'race_White'] TPR: 0.5581317204301075\n",
      "['gender_Male', 'race_Black'] TPR: 0.48175182481751827\n"
     ]
    }
   ],
   "source": [
    "subsets = [\n",
    "    ['gender_Female'], ['gender_Male'], ['race_White'], ['race_Black'],\n",
    "    ['gender_Female', 'race_White'],\n",
    "    ['gender_Female', 'race_Black'],\n",
    "    ['gender_Male', 'race_White'],\n",
    "    ['gender_Male', 'race_Black']\n",
    "]\n",
    "for subset in subsets:\n",
    "    acc = get_acc_subset(test_df, subset)\n",
    "    print(subset, \"Accuracy:\", acc)\n",
    "print()\n",
    "for subset in subsets:\n",
    "    tpr_val = get_tpr_subset(test_df, subset)\n",
    "    print(subset, \"TPR:\", tpr_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving using Best Iterate instead of Last Iterate.\n",
    "\n",
    "As discussed in [[CotterEtAl18b]](https://arxiv.org/abs/1809.04198), the last iterate may not be the best choice and suggests a simple heuristic to choose the best iterate out of the ones found after each epoch. The heuristic proceeds by ranking each of the solutions based on accuracy and fairness separately with respect to the training data. Any solutions which satisfy the constraints are equally ranked top in terms fairness. Each solution thus has two ranks. Then, the chosen solution is the one with the smallest maximum of the two ranks. We see that this improves the fairness and can find a better accuracy / fairness trade-off on the training data. \n",
    "\n",
    "This solution can be calculated using find_best_candidate_index given the list of training errors and violations associated with each of the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.14133472559196586\n",
      "Train Violation -0.004157023292723272\n",
      "\n",
      "Test Error 0.14348013021313188\n",
      "Test Violation 0.01559670208037367\n"
     ]
    }
   ],
   "source": [
    "best_cand_index = tfco.find_best_candidate_index(train_errors, train_violations)\n",
    "\n",
    "print(\"Train Error\", train_errors[best_cand_index])\n",
    "print(\"Train Violation\", max(train_violations[best_cand_index]))\n",
    "print()\n",
    "print(\"Test Error\", test_errors[best_cand_index])\n",
    "print(\"Test Violation\", max(test_violations[best_cand_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using stochastic solutions.\n",
    "\n",
    "As discussed in [[CoJiSr19]](https://arxiv.org/abs/1804.06500), neither the best nor last iterate will come with theoretical guarantees. One can instead use randomized solutions, which come with theoretical guarantees. However, as discussed in [[CotterEtAl18b]](https://arxiv.org/abs/1809.04198), there may not always be a clear practical benefits. We show how to use these solutions here for sake of completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-stochastic solution.\n",
    "The first and simplest randomized solution suggested is the T-stochastic, which simply takes the average of all of the iterates found at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.14204109210405086\n",
      "Train Violation 0.00040501346694489656\n",
      "\n",
      "Test Error 0.14335882316811008\n",
      "Test Violation 0.021693837027224078\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error\", np.mean(train_errors))\n",
    "print(\"Train Violation\", max(np.mean(train_violations, axis=0)))\n",
    "print()\n",
    "print(\"Test Error\", np.mean(test_errors))\n",
    "print(\"Test Violation\", max(np.mean(test_violations, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m-stochastic solution.\n",
    "[[CoJiSr19]](https://arxiv.org/abs/1804.06500) presents a method which shrinks down the T-stochastic solution down to one that is supported on at most (m+1) points where m is the number of constraints and is guaranteed to be at least as good as the T-stochastic solution. Here we see that indeed there is benefit in performing the shrinking.\n",
    "\n",
    "This solution can be computed using find_best_candidate_distribution by passing in the training errors and violations found at each epoch and returns the weight of each constituent. We see that indeed, it is sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.27921444\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.72078556 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "cand_dist = tfco.find_best_candidate_distribution(train_errors, train_violations)\n",
    "print(cand_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.14129184999836308\n",
      "Train Violation 1.470178145890344e-16\n",
      "\n",
      "Test Error 0.14330863310567057\n",
      "Test Violation 0.01799448570692895\n"
     ]
    }
   ],
   "source": [
    "m_stoch_error_train, m_stoch_violations_train = _get_exp_error_rate_constraints(cand_dist, train_errors, train_violations)\n",
    "m_stoch_error_test, m_stoch_violations_test = _get_exp_error_rate_constraints(cand_dist, test_errors, test_violations)\n",
    "\n",
    "print(\"Train Error\", m_stoch_error_train)\n",
    "print(\"Train Violation\", max(m_stoch_violations_train))\n",
    "print()\n",
    "print(\"Test Error\", m_stoch_error_test)\n",
    "print(\"Test Violation\", max(m_stoch_violations_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
